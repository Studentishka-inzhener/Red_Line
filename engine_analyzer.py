#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
engine_analyzer.py

–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å (MLP) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏–∑–∏–∫–æ-–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏ —Ä–∞–∫–µ—Ç–Ω—ã—Ö –¥–≤–∏–≥–∞—Ç–µ–ª–µ–π.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç supervised (regression/classification) –∏ unsupervised (autoencoder –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π).
–ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: –≤—ã–±–æ—Ä —Ç–∏–ø–∞ –¥–≤–∏–≥–∞—Ç–µ–ª—è –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏ (CSV/JSON/NPZ/MAT/TXT).

–ê–≤—Ç–æ—Ä: denis-tsar
–î–∞—Ç–∞: 15.11.2025
"""

import os
import sys
import argparse
import json
import math
import time
from typing import List, Tuple, Optional, Dict

import numpy as np
import pandas as pd
from scipy import signal
from scipy.fft import rfft, rfftfreq
from scipy.io import loadmat
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, roc_auc_score
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import scipy.stats as stats
from multiprocessing import Pool, cpu_count

# -----------------------------
# GPU-accelerated feature extraction (batch mode)
# -----------------------------

def _process_unit_worker(args):
    """
    Worker –¥–ª—è multiprocessing.
    –ö–∞–∂–¥–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å—É –ø–µ—Ä–µ–¥–∞—ë–º:
        unit_df, window_size, step, sensors_only, mode, batch_size, device
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (X_unit, y_unit, meta_unit)
    """
    (unit_df,
     window_size,
     step,
     sensors_only,
     mode,
     batch_size,
     device) = args

    # 1) –°–±–æ—Ä –æ–∫–æ–Ω (–ª–æ–∫–∞–ª—å–Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ)
    windows_np, y_arr, meta = build_windows_matrix_from_df(
        unit_df,
        window_size=window_size,
        step=step,
        sensors_only=sensors_only,
        mode=mode
    )

    if windows_np.size == 0:
        return np.zeros((0,0)), np.array([]), []

    # 2) GPU batch feature extraction (–ª–æ–∫–∞–ª—å–Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ)
    X_unit = []
    N = windows_np.shape[0]
    bs = min(batch_size, N)

    for i in range(0, N, bs):
        batch = windows_np[i:i+bs]
        feats = feature_extractor_gpu_batch(batch, device=device)
        X_unit.append(feats)

    X_unit = np.vstack(X_unit)
    y_unit = y_arr.copy()
    return X_unit, y_unit, meta

def build_cmapss_window_dataset_parallel(
        df: pd.DataFrame,
        window_size: int = 30,
        step: int = 1,
        sensors_only: bool = True,
        mode: str = 'sliding',
        batch_size: int = 2048,
        n_workers: int = None,
        device: str = None):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ C-MAPSS –ø–æ unit.
    
    df: DataFrame —Å —Å–µ–Ω—Å–æ—Ä–∞–º–∏ + RUL
    device: 'cuda', 'cpu' –∏–ª–∏ None (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
    n_workers: –∫–æ–ª-–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤; –µ—Å–ª–∏ None ‚Äî cpu_count()-1

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        X (N,F)
        y (N,)
        meta (list)
    """

    # –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device)

    # –≤—ã–±–æ—Ä —á–∏—Å–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
    if n_workers is None:
        n_workers = max(1, cpu_count() - 1)

    # —Ä–∞–∑–±–∏–≤–∞–µ–º df –ø–æ —é–Ω–∏—Ç–∞–º (–ø—Ä–æ—Ü–µ—Å—Å—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –¥–µ–ª–∏—Ç—å –ø–∞–º—è—Ç—å df)
    unit_groups = [g for _, g in df.groupby('unit')]

    # —Å–æ–±–∏—Ä–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
    worker_args = [
        (g,
         window_size,
         step,
         sensors_only,
         mode,
         batch_size,
         device)
        for g in unit_groups
    ]

    # –∑–∞–ø—É—Å–∫–∞–µ–º multiprocessing
    with Pool(processes=n_workers) as pool:
        results = pool.map(_process_unit_worker, worker_args)

    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    X_all = []
    y_all = []
    meta_all = []

    for X_unit, y_unit, meta_unit in results:
        if X_unit.size == 0:
            continue
        X_all.append(X_unit)
        y_all.append(y_unit)
        meta_all.extend(meta_unit)

    if not X_all:
        return np.zeros((0,0)), np.array([]), []

    X_all = np.vstack(X_all)
    y_all = np.concatenate(y_all)

    return X_all, y_all, meta_all


def build_windows_matrix_from_df(df: pd.DataFrame, window_size: int = 30, step: int = 1,
                                 sensors_only: bool = True, mode: str = 'sliding'):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –æ–∫–Ω–∞ –∏–∑ df –≤ –æ–¥–∏–Ω –±–∞—Ç—á numpy –º–∞—Å—Å–∏–≤ (N, T, C),
    –∞ —Ç–∞–∫–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç targets –∏ meta.
    –≠—Ç–æ CPU-–æ–ø–µ—Ä–∞—Ü–∏—è (—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫–æ–Ω –≤ –ø–∞–º—è—Ç–∏).
    """
    sensor_cols = [c for c in df.columns if c.startswith('sensor_')]
    if sensors_only:
        feat_cols = sensor_cols
    else:
        setting_cols = [c for c in df.columns if c.startswith('op_setting_')]
        feat_cols = setting_cols + sensor_cols

    windows = []
    targets = []
    metas = []

    for unit, g in df.groupby('unit'):
        g = g.sort_values('cycle').reset_index(drop=True)
        Xmat = g[feat_cols].values.astype(float)
        RUL = g['RUL'].values.astype(float)
        L = len(g)
        T = window_size
        if L < T:
            continue
        if mode == 'sliding':
            for start in range(0, L - T + 1, step):
                end = start + T
                windows.append(Xmat[start:end, :])            # (T, C)
                targets.append(float(RUL[end - 1]))
                metas.append({'unit': int(unit), 'cycle': int(g.loc[end - 1, 'cycle'])})
        elif mode == 'per_cycle':
            for idx in range(T - 1, L):
                start = idx - T + 1
                windows.append(Xmat[start:idx+1, :])
                targets.append(float(RUL[idx]))
                metas.append({'unit': int(unit), 'cycle': int(g.loc[idx, 'cycle'])})
        else:
            raise ValueError("mode must be 'sliding' or 'per_cycle'")

    if len(windows) == 0:
        return np.zeros((0, window_size, 0)), np.array([]), []
    W = np.stack(windows, axis=0)  # (N, T, C)
    return W, np.array(targets, dtype=float), metas

def feature_extractor_gpu_batch(windows_np: np.ndarray, device: Optional[torch.device] = None,
                                last_ns: List[int] = [5,10,20]):
    """
    –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π GPU feature extractor.
    Inputs:
      windows_np: numpy array shape (N, T, C)
      device: torch.device('cuda') or ('cpu') ; if None => cuda if available else cpu
    Returns:
      X_feats: numpy array shape (N, F)
    –ü—Ä–∏–º.: —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ç–µ –∂–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, —á—Ç–æ –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π feature_extractor,
    –Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç –∏—Ö –±–∞—Ç—á–µ–≤–æ —á–µ—Ä–µ–∑ PyTorch –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è.
    """
    if windows_np.size == 0:
        return np.zeros((0, 0), dtype=float)

    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # move to torch
    W = torch.from_numpy(windows_np.astype(np.float32)).to(device)  # (N, T, C)
    N, T, C = W.shape

    # --- 1) basic stats per channel ---
    # mean, std, median, min, max, ptp, rms, q25, q75, iqr, skew, kurt (per channel)
    mean = W.mean(dim=1)                     # (N, C)
    std = W.std(dim=1, unbiased=False)       # (N, C)
    median = W.median(dim=1).values          # (N, C)
    mn = W.min(dim=1).values                 # (N, C)
    mx = W.max(dim=1).values                 # (N, C)
    ptp = mx - mn

# -----------------------------
# Improved feature extractor for C-MAPSS (recommended)
# -----------------------------
def feature_extractor(window: np.ndarray) -> np.ndarray:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è C-MAPSS.
    –û–∂–∏–¥–∞–µ—Ç—Å—è –≤—Ö–æ–¥: window shape = (T, C) ‚Äî —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏:
      1) –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É —Å–µ–Ω—Å–æ—Ä—É
      2) –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ (1-—è –∏ 2-—è)
      3) –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (rms, ptp, iqr)
      4) –°–∫–æ–ª—å–∑—è—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —à–∞–≥–æ–≤ –≤–Ω—É—Ç—Ä–∏ –æ–∫–Ω–∞
      5) –¢—Ä–µ–Ω–¥ (–ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –Ω–∞–∫–ª–æ–Ω–∞)
      6) FFT —ç–Ω–µ—Ä–≥–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö
      7) –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –æ–∫–Ω–∞)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ 1D numpy array.
    """

    T, C = window.shape
    feats = []

    # ============================================================
    # üîπ 1. –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–∞–Ω–∞–ª—É
    # ============================================================
    for ch in range(C):
        x = window[:, ch].astype(float)

        mean = x.mean()
        std = x.std() if x.size > 1 else 0.
        median = np.median(x)
        mn = x.min()
        mx = x.max()
        ptp = mx - mn
        rms = np.sqrt(np.mean(x**2))
        q25, q75 = np.percentile(x, [25, 75])
        iqr = q75 - q25

        try:
            skew = stats.skew(x)
            kurt = stats.kurtosis(x)
        except:
            skew, kurt = 0.0, 0.0

        feats += [
            mean, std, median, mn, mx,
            ptp, rms, q25, q75, iqr,
            skew, kurt
        ]

    # ============================================================
    #  2. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ (gradient)
    # ============================================================
    for ch in range(C):
        x = window[:, ch].astype(float)
        dx = np.gradient(x)
        ddx = np.gradient(dx)
        feats += [
            dx.mean(), dx.std(),
            ddx.mean(), ddx.std()
        ]

    # ============================================================
    #  3. –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —à–∞–≥–æ–≤ (–ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã RUL)
    # ============================================================

    # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5, 10 –∏ 20 —Ü–∏–∫–ª–æ–≤
    for last_n in [5, 10, 20]:
        if T >= last_n:
            x_last = window[-last_n:, :]
            # mean/std –ø–æ –∫–∞–∂–¥–æ–º—É –¥–∞—Ç—á–∏–∫—É –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —Ü–∏–∫–ª–æ–≤
            feats.append(x_last.mean())
            feats.append(x_last.std())
        else:
            feats += [0.0, 0.0]

    # ============================================================
    #  4. –¢—Ä–µ–Ω–¥—ã (–ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –Ω–∞–∫–ª–æ–Ω–∞)
    # ============================================================
    # slope –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (–≥–ª–∞–≤–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ degradation)
    t_idx = np.arange(T)
    for ch in range(C):
        x = window[:, ch].astype(float)
        if np.all(x == x[0]):
            slope = 0.0
        else:
            # –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è: slope = cov(t,x)/var(t)
            slope = np.cov(t_idx, x)[0, 1] / np.var(t_idx)
        feats.append(slope)

    # ============================================================
    #  5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –æ–∫–Ω–∞
    # ============================================================
    for ch in range(C):
        x = window[:, ch]
        base = x[0] if x[0] != 0 else 1e-6
        feats.append((x[-1] - x[0]) / base)     # –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        feats.append(x[-1] / base)              # –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å

    # ============================================================
    #  6. FFT ENERGY
    # ============================================================
    for ch in range(C):
        x = window[:, ch].astype(float)
        x = x - x.mean()
        if T < 8:
            feats += [0., 0., 0., 0., 0.]
            continue

        yf = np.abs(rfft(x))
        L = len(yf)
        nb = 4
        band = L // nb
        for b in range(nb):
            s = b * band
            e = min(L, (b + 1) * band)
            feats.append(yf[s:e].sum())
        feats.append(yf.sum())  # total energy

    # ============================================================
    # –í–æ–∑–≤—Ä–∞—Ç 1D numpy –º–∞—Å—Å–∏–≤–∞
    # ============================================================
    return np.array(feats, dtype=float)

def cap_rul(df: pd.DataFrame, max_rul: int = 125) -> pd.DataFrame:
    """
    –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç RUL —Å–≤–µ—Ä—Ö—É. –°—Ç–∞–Ω–¥–∞—Ä—Ç NASA: cap = 125.
    """
    df = df.copy()
    df['RUL'] = df['RUL'].clip(upper=max_rul)
    return df

def soft_cap_rul(df: pd.DataFrame,
                 alpha: float = 40.0,
                 beta: float = 20.0,
                 column: str = 'RUL') -> pd.DataFrame:
    """
    Probabilistic soft-cap –¥–ª—è NASA C-MAPSS RUL.
    –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ —Å–∂–∞—Ç–∏–µ:
        RUL_new = alpha * log(1 + RUL / beta)
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
        - –Ω–µ—Ç –∂—ë—Å—Ç–∫–æ–π –æ—Ç—Å–µ—á–∫–∏
        - —É–º–µ–Ω—å—à–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –æ–≥—Ä–æ–º–Ω—ã—Ö RUL
        - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—è–¥–æ–∫ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏
    """
    df = df.copy()
    
    x = df[column].astype(float)
    df[column] = alpha * np.log1p(x / beta)
    
    return df

# -----------------------------
# New / replacement functions for NASA C-MAPSS
# -----------------------------
def read_cmapss_file(path: str, n_settings: int = 3) -> pd.DataFrame:
    """
    –ß–∏—Ç–∞–µ—Ç C-MAPSS FDxxx —Ñ–∞–π–ª (space separated) –≤ DataFrame.
    –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏—Å–≤–æ–∏—Ç—å –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞–º:
      ['unit', 'cycle'] + ['op_setting_1'..] + ['sensor_1'..]
    n_settings: –æ–∂–∏–¥–∞–µ–º–æ–µ —á–∏—Å–ª–æ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ (–æ–±—ã—á–Ω–æ 3)
    """
    # C-MAPSS —Ä–∞–∑–¥–µ–ª—ë–Ω –ø—Ä–æ–±–µ–ª–∞–º–∏ / –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏
    df = pd.read_csv(path, sep=r'\s+', header=None, engine='python')
    n_cols = df.shape[1]
    if n_cols < 5:
        raise ValueError(f"–§–∞–π–ª {path} –∏–º–µ–µ—Ç –Ω–µ–æ–∂–∏–¥–∏–º–æ–µ —á–∏—Å–ª–æ –∫–æ–ª–æ–Ω–æ–∫: {n_cols}")
    # 2 –∫–æ–ª–æ–Ω–∫–∏: unit, cycle; –¥–∞–ª–µ–µ n_settings –∏ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è - —Å–µ–Ω—Å–æ—Ä—ã
    if n_cols - 2 < n_settings + 1:
        # –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–Ω–æ–µ —á–∏—Å–ª–æ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å–ª–∏—à–∫–æ–º –≤–µ–ª–∏–∫–æ, –ø–æ–Ω–∏–∑–∏–º
        n_settings = max(0, n_cols - 3)
    n_sensors = n_cols - 2 - n_settings
    cols = ['unit', 'cycle'] + [f'op_setting_{i+1}' for i in range(n_settings)] + [f'sensor_{i+1}' for i in range(n_sensors)]
    df.columns = cols
    # ensure proper dtypes
    df['unit'] = df['unit'].astype(int)
    df['cycle'] = df['cycle'].astype(int)
    return df

def read_rul_file(path: str) -> np.ndarray:
    """
    –ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª RUL (test FDxxx_RUL.txt) ‚Äî –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞/—á—É—Ç—å –±–æ–ª—å—à–µ:
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç 1D numpy array length = number of units (rul for last cycle of each unit).
    """
    arr = np.loadtxt(path, dtype=float)
    # If file is a single column, shape OK; if multiple columns pick first
    if arr.ndim > 1:
        arr = arr[:, 0]
    return np.array(arr, dtype=float)

def attach_rul_train(df: pd.DataFrame,
                     alpha: float = 40.0,
                     beta: float = 20.0) -> pd.DataFrame:
    df = df.copy()
    max_cycle = df.groupby('unit')['cycle'].transform('max')
    df['RUL'] = (max_cycle - df['cycle']).astype(float)

    # soft-cap
    df = soft_cap_rul(df, alpha=alpha, beta=beta)
    return df

def attach_rul_test(df: pd.DataFrame,
                    rul_arr: np.ndarray,
                    alpha: float = 40.0,
                    beta: float = 20.0):
    df = df.copy()
    
    units = np.sort(df['unit'].unique())
    if len(rul_arr) != len(units):
        raise ValueError("rul_arr size doesn't match unit count!")

    rul_map = {u: float(rul_arr[i]) for i, u in enumerate(units)}
    max_cycle = df.groupby('unit')['cycle'].transform('max')

    df['RUL'] = df.apply(
        lambda r: rul_map[int(r['unit'])] + (max_cycle.loc[r.name] - r['cycle']),
        axis=1
    ).astype(float)

    # apply soft-cap
    df = soft_cap_rul(df, alpha=alpha, beta=beta)
    return df

# ---------------------------------------------------------
# NEW build_cmapss_window_dataset with improved feature_extractor hook
# ---------------------------------------------------------
def build_cmapss_window_dataset(
        df: pd.DataFrame,
        window_size: int = 30,
        step: int = 1,
        sensors_only: bool = True,
        feature_extractor=None,
        mode: str = 'sliding'
    ) -> Tuple[np.ndarray, np.ndarray, List[dict]]:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç X, y, meta –¥–ª—è C-MAPSS.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
      - window_size: –¥–ª–∏–Ω–∞ –æ–∫–Ω–∞ (T)
      - step: —à–∞–≥ –æ–∫–Ω–∞
      - sensors_only: True ‚Üí –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ sensor_*, False ‚Üí –¥–æ–±–∞–≤–ª—è–µ–º op_setting_*
      - feature_extractor: —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –Ω–æ–≤—É—é)
      - mode: 'sliding' (–≤—Å–µ –æ–∫–Ω–∞) –∏–ª–∏ 'per_cycle' (–æ–∫–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ü–∏–∫–ª–∞ –ø–æ—Å–ª–µ T)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      X: (N, F) ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
      y: (N,) ‚Äî RUL –ø–æ—Å–ª–µ soft-cap
      meta: —Å–ø–∏—Å–æ–∫ dict: unit, cycle
    """

    # ------------------------------------------
    # –í–ê–ñ–ù–û: –∂—ë—Å—Ç–∫–æ –ø—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ–≤—ã–π feature_extractor
    # ------------------------------------------
    if feature_extractor is None:
        raise ValueError("ERROR: –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å –Ω–æ–≤—ã–π feature_extractor!")

    sensor_cols = [c for c in df.columns if c.startswith('sensor_')]

    if sensors_only:
        feat_cols = sensor_cols
    else:
        setting_cols = [c for c in df.columns if c.startswith('op_setting_')]
        feat_cols = setting_cols + sensor_cols

    samples = []
    labels = []
    metas = []

    # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–≤–∏–≥–∞—Ç–µ–ª—è–º (unit)
    for unit, g in df.groupby('unit'):
        g = g.sort_values('cycle').reset_index(drop=True)

        Xmat = g[feat_cols].values.astype(float)
        RUL = g['RUL'].values.astype(float)

        L = len(g)
        T = window_size

        if L < T:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            continue

        if mode == 'sliding':
            # –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ–∫–Ω–∞
            for start in range(0, L - T + 1, step):
                end = start + T
                window = Xmat[start:end, :]  # (T, C)

                feats = feature_extractor(window)

                # —Ç–∞—Ä–≥–µ—Ç = RUL –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ü–∏–∫–ª–∞ –æ–∫–Ω–∞
                target = float(RUL[end - 1])
                cycle = int(g.loc[end - 1, 'cycle'])

                samples.append(feats)
                labels.append(target)
                metas.append({'unit': int(unit), 'cycle': cycle})

        elif mode == 'per_cycle':
            # –æ–∫–Ω–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ü–∏–∫–ª–∞ >= window_size
            for idx in range(T - 1, L):
                start = idx - T + 1
                window = Xmat[start:idx + 1, :]

                feats = feature_extractor(window)
                target = float(RUL[idx])
                cycle = int(g.loc[idx, 'cycle'])

                samples.append(feats)
                labels.append(target)
                metas.append({'unit': int(unit), 'cycle': cycle})

        else:
            raise ValueError("mode must be 'sliding' or 'per_cycle'")

    if not samples:
        raise ValueError("–ù–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞. –£–º–µ–Ω—å—à–∏ window_size –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å –¥–∞–Ω–Ω—ã–µ.")

    X = np.vstack(samples).astype(float)
    y = np.array(labels).astype(float)

    return X, y, metas

def prepare_cmapss_supervised(train_path: str,
                              test_path: str,
                              test_rul_path: str,
                              n_settings: int = 3,
                              window_size: int = 30,
                              step: int = 1,
                              sensors_only: bool = True,
                              feature_extractor=None,
                              scale: bool = True,
                              alpha: float = 40.0,
                              beta: float = 20.0):
    """
    NASA C-MAPSS —Å probabilistic soft-cap RUL (logarithmic compression)
    """

    df_train = read_cmapss_file(train_path, n_settings=n_settings)
    df_test = read_cmapss_file(test_path, n_settings=n_settings)
    rul_arr = read_rul_file(test_rul_path)

    # soft-cap RUL
    df_train = attach_rul_train(df_train, alpha=alpha, beta=beta)
    df_test = attach_rul_test(df_test, rul_arr, alpha=alpha, beta=beta)

    X_tr, y_tr, meta_tr = build_cmapss_window_dataset(
        df_train, window_size, step,
        sensors_only=sensors_only,
        feature_extractor=feature_extractor
    )

    X_te, y_te, meta_te = build_cmapss_window_dataset(
        df_test, window_size, step,
        sensors_only=sensors_only,
        feature_extractor=feature_extractor
    )

    scaler = None
    if scale:
        scaler = StandardScaler()
        X_tr = scaler.fit_transform(X_tr)
        X_te = scaler.transform(X_te)

    return {
        "X_train": X_tr, "y_train": y_tr, "meta_train": meta_tr,
        "X_test": X_te, "y_test": y_te, "meta_test": meta_te,
        "scaler": scaler
    }

# -----------------------------
# Utilities for file parsing
# -----------------------------
def load_file_generic(path: str) -> dict:
    """
    –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª–∏. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å:
    {
      'data': np.ndarray (2D: samples x channels/time or 1D vector),
      'meta': dict (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    }
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: .csv, .json, .npz, .npy, .mat, .txt
    """
    ext = os.path.splitext(path)[1].lower()
    result = {'data': None, 'meta': {}}
    if ext == '.csv':
        df = pd.read_csv(path)
        result['data'] = df.values.astype(float)
        result['meta'] = {'columns': list(df.columns)}
    elif ext == '.json':
        with open(path, 'r', encoding='utf-8') as f:
            js = json.load(f)
        # –û–∂–∏–¥–∞–µ–º –ª–∏–±–æ {"data": [...]} –ª–∏–±–æ —Ç–∞–±–ª–∏—á–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤
        if isinstance(js, dict) and 'data' in js:
            arr = np.array(js['data'], dtype=float)
            result['data'] = arr
            result['meta'] = {k: v for k, v in js.items() if k != 'data'}
        elif isinstance(js, list):
            df = pd.json_normalize(js)
            result['data'] = df.values.astype(float)
            result['meta'] = {'columns': list(df.columns)}
        else:
            raise ValueError("JSON —Ñ–æ—Ä–º–∞—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è: –æ–∂–∏–¥–∞–µ—Ç—Å—è –º–∞—Å—Å–∏–≤ –∏–ª–∏ –∫–ª—é—á 'data'")
    elif ext in ('.npz', '.npy'):
        arr = np.load(path, allow_pickle=True)
        if isinstance(arr, np.lib.npyio.NpzFile):
            # –±—Ä–∞—Ç—å –ø–µ—Ä–≤—ã–π –º–∞—Å—Å–∏–≤
            k = list(arr.files)[0]
            result['data'] = arr[k].astype(float)
        else:
            result['data'] = arr.astype(float)
    elif ext == '.mat':
        mat = loadmat(path)
        # –≤–∑—è—Ç—å –ø–µ—Ä–≤–æ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –ø–æ–ª–µ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        for k, v in mat.items():
            if k.startswith('__'):
                continue
            if isinstance(v, np.ndarray):
                result['data'] = v.astype(float)
                result['meta'] = {'mat_key': k}
                break
        if result['data'] is None:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω –ø—Ä–∏–≥–æ–¥–Ω—ã–π –º–∞—Å—Å–∏–≤ –≤ .mat —Ñ–∞–π–ª–µ")
    elif ext in ('.txt', '.dat'):
        arr = np.loadtxt(path)
        result['data'] = arr.astype(float)
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {ext}")
    # Ensure 2D
    if result['data'] is not None:
        arr = np.array(result['data'])
        if arr.ndim == 1:
            arr = arr.reshape(-1, 1)
        result['data'] = arr
    return result

# -----------------------------
# Feature extraction
# -----------------------------
def extract_time_series_features(arr: np.ndarray, sr: Optional[float] = None) -> np.ndarray:
    """
    arr: 2D array (T x channels) - –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –∏–ª–∏ 1D vector shaped as (T,1)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç 1D –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    –ò–∑–≤–ª–µ–∫–∞—é—Ç—Å—è:
      - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–∞–Ω–∞–ª—É: mean, std, median, min, max, skew, kurtosis (approx)
      - –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ (first, second) —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
      - —ç–Ω–µ—Ä–≥–∏—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö FFT-–¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö
      - peak-to-peak, rms
    """
    import scipy.stats as stats
    T, C = arr.shape
    feats = []
    for ch in range(C):
        x = arr[:, ch].astype(float)
        # basic stats
        mean = x.mean()
        std = x.std(ddof=0)
        med = np.median(x)
        mn = x.min()
        mx = x.max()
        ptp = mx - mn
        rms = math.sqrt(np.mean(x**2))
        # robust measures
        q25, q75 = np.percentile(x, [25, 75])
        iqr = q75 - q25
        # skew/kurtosis (use scipy)
        try:
            skew = float(stats.skew(x))
            kurt = float(stats.kurtosis(x))
        except Exception:
            skew, kurt = 0.0, 0.0
        feats += [mean, std, med, mn, mx, ptp, rms, q25, q75, iqr, skew, kurt]
        # derivatives
        dx = np.gradient(x)
        ddx = np.gradient(dx)
        feats += [dx.mean(), dx.std(), ddx.mean(), ddx.std()]

        # FFT energy bands
        n = len(x)
        if n >= 8:
            yf = np.abs(rfft(x - mean))
            freqs = rfftfreq(n, d=1.0 if sr is None else 1.0/sr)
            # define bands relative to Nyquist: split into 4 bands
            nb = 4
            L = len(yf)
            band_size = max(1, L // nb)
            for b in range(nb):
                s = b * band_size
                e = min(L, (b + 1) * band_size)
                band_energy = yf[s:e].sum()
                feats.append(band_energy)
            feats.append(yf.sum())  # total energy
        else:
            feats += [0.0] * 5
    # flatten to numpy
    return np.array(feats, dtype=float)

def extract_features_from_sample(sample: np.ndarray) -> np.ndarray:
    """
    sample: 2D or 1D ndarray representing a single model output / time series / parameter set
    If sample has many rows (time), treat as time-series; else treat each column as feature.
    """
    arr = np.array(sample)
    if arr.ndim == 1:
        arr = arr.reshape(-1, 1)
    # If time dimension large (>10), consider time-series features
    if arr.shape[0] >= 10:
        return extract_time_series_features(arr)
    else:
        # If small, compute simple stats across columns
        v = arr.flatten()
        feats = [
            v.mean(),
            v.std(ddof=0) if v.size > 1 else 0.0,
            v.min(),
            v.max(),
            np.median(v),
            np.percentile(v, 25) if v.size > 1 else v[0],
            np.percentile(v, 75) if v.size > 1 else v[0],
        ]
        return np.array(feats, dtype=float)

# -----------------------------
# Dataset
# -----------------------------
class EngineDataset(Dataset):
    """
    –•—Ä–∞–Ω–∏–ª–∏—â–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫.
    inputs: ndarray (N x F)
    labels: ndarray (N x ...) or None
    meta: list of dicts
    """
    def __init__(self, inputs: np.ndarray, labels: Optional[np.ndarray] = None, meta: Optional[List[dict]] = None):
        self.X = inputs.astype(np.float32)
        self.y = labels.astype(np.float32) if labels is not None else None
        self.meta = meta if meta is not None else [None] * len(self.X)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        if self.y is None:
            return self.X[idx], self.meta[idx]
        else:
            return self.X[idx], self.y[idx], self.meta[idx]

# -----------------------------
# Models: MLP and Autoencoder
# -----------------------------
class MLP(nn.Module):
    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int, activation: str = 'relu', dropout: float = 0.0):
        super().__init__()
        act = {'relu': nn.ReLU, 'tanh': nn.Tanh, 'gelu': nn.GELU}.get(activation, nn.ReLU)
        layers = []
        prev = input_dim
        for h in hidden_dims:
            layers.append(nn.Linear(prev, h))
            layers.append(act())
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            prev = h
        layers.append(nn.Linear(prev, output_dim))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

class Autoencoder(nn.Module):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dims: List[int], activation: str = 'relu'):
        super().__init__()
        act = {'relu': nn.ReLU, 'tanh': nn.Tanh, 'gelu': nn.GELU}.get(activation, nn.ReLU)
        # encoder
        enc_layers = []
        prev = input_dim
        for h in hidden_dims:
            enc_layers.append(nn.Linear(prev, h)); enc_layers.append(act()); prev = h
        enc_layers.append(nn.Linear(prev, latent_dim))
        self.encoder = nn.Sequential(*enc_layers)
        # decoder (mirror)
        dec_layers = []
        prev = latent_dim
        for h in reversed(hidden_dims):
            dec_layers.append(nn.Linear(prev, h)); dec_layers.append(act()); prev = h
        dec_layers.append(nn.Linear(prev, input_dim))
        self.decoder = nn.Sequential(*dec_layers)

    def forward(self, x):
        z = self.encoder(x)
        xrec = self.decoder(z)
        return xrec

# -----------------------------
# Training & evaluation helpers
# -----------------------------
def train_supervised(model: nn.Module,
                     train_loader: DataLoader,
                     val_loader: DataLoader,
                     device: torch.device,
                     epochs: int = 100,
                     lr: float = 1e-3,
                     weight_decay: float = 0.0,
                     task: str = 'regression',
                     early_stop_patience: int = 10,
                     checkpoint_path: str = 'best_supervised.pth'):
    """
    task: 'regression' or 'classification'
    """
    model.to(device)
    if task == 'regression':
        criterion = nn.MSELoss()
    else:
        # For classification binary or multi-class: use BCE or CrossEntropy based on labels shape
        criterion = nn.CrossEntropyLoss()

    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

    best_val = float('inf')
    best_epoch = -1
    history = {'train_loss': [], 'val_loss': []}

    for epoch in range(1, epochs + 1):
        model.train()
        train_losses = []
        for batch in train_loader:
            if task == 'regression':
                x, y, *_ = batch
                y = y.to(device)
            else:
                x, y, *_ = batch
                y = y.long().to(device)
            x = x.to(device)
            optimizer.zero_grad()
            pred = model(x)
            loss = criterion(pred.squeeze(), y) if task == 'regression' else criterion(pred, y)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())
        train_loss = float(np.mean(train_losses)) if train_losses else 0.0

        # validation
        model.eval()
        val_losses = []
        with torch.no_grad():
            for batch in val_loader:
                if task == 'regression':
                    x, y, *_ = batch
                    y = y.to(device)
                else:
                    x, y, *_ = batch
                    y = y.long().to(device)
                x = x.to(device)
                pred = model(x)
                loss = criterion(pred.squeeze(), y) if task == 'regression' else criterion(pred, y)
                val_losses.append(loss.item())
        val_loss = float(np.mean(val_losses)) if val_losses else 0.0
        scheduler.step(val_loss)
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)

        print(f"[Epoch {epoch:03d}] train_loss={train_loss:.6f}  val_loss={val_loss:.6f}")

        # early stopping & checkpoint
        if val_loss < best_val - 1e-9:
            best_val = val_loss
            best_epoch = epoch
            torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict()}, checkpoint_path)
        if epoch - best_epoch >= early_stop_patience:
            print("Early stopping triggered.")
            break

    # load best
    if os.path.exists(checkpoint_path):
        ckpt = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(ckpt['model_state'])
    return model, history

def train_autoencoder(ae: Autoencoder,
                      train_loader: DataLoader,
                      val_loader: DataLoader,
                      device: torch.device,
                      epochs: int = 100,
                      lr: float = 1e-3,
                      weight_decay: float = 0.0,
                      early_stop_patience: int = 10,
                      checkpoint_path: str = 'best_autoencoder.pth'):
    ae.to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

    best_val = float('inf'); best_epoch = -1
    history = {'train_loss': [], 'val_loss': []}
    for epoch in range(1, epochs + 1):
        ae.train()
        t_losses = []
        for batch in train_loader:
            x, *_ = batch
            x = x.to(device)
            optimizer.zero_grad()
            xrec = ae(x)
            loss = criterion(xrec, x)
            loss.backward()
            optimizer.step()
            t_losses.append(loss.item())
        train_loss = float(np.mean(t_losses)) if t_losses else 0.0

        # validation
        ae.eval()
        v_losses = []
        with torch.no_grad():
            for batch in val_loader:
                x, *_ = batch
                x = x.to(device)
                xrec = ae(x)
                loss = criterion(xrec, x)
                v_losses.append(loss.item())
        val_loss = float(np.mean(v_losses)) if v_losses else 0.0
        scheduler.step(val_loss)
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        print(f"[AE Epoch {epoch:03d}] train_loss={train_loss:.6f} val_loss={val_loss:.6f}")
        if val_loss < best_val - 1e-9:
            best_val = val_loss; best_epoch = epoch
            torch.save({'epoch': epoch, 'model_state': ae.state_dict(), 'optimizer_state': optimizer.state_dict()}, checkpoint_path)
        if epoch - best_epoch >= early_stop_patience:
            print("Early stopping AE.")
            break
    if os.path.exists(checkpoint_path):
        ckpt = torch.load(checkpoint_path, map_location=device)
        ae.load_state_dict(ckpt['model_state'])
    return ae, history

# -----------------------------
# Evaluation & plotting
# -----------------------------
def evaluate_supervised(model: nn.Module, loader: DataLoader, device: torch.device, task: str = 'regression'):
    model.eval()
    preds = []
    targets = []
    metas = []
    with torch.no_grad():
        for batch in loader:
            if task == 'regression':
                x, y, meta = batch
                targets.append(y.numpy())
            else:
                x, y, meta = batch
                targets.append(y.numpy())
            x = x.to(device)
            out = model(x).cpu().numpy()
            preds.append(out)
            metas += list(meta)
    preds = np.vstack(preds)
    targets = np.vstack(targets).squeeze()
    return preds, targets, metas

def evaluate_autoencoder(ae: Autoencoder, loader: DataLoader, device: torch.device):
    ae.eval()
    rec_errors = []
    metas = []
    with torch.no_grad():
        for batch in loader:
            x, meta = batch
            x = x.to(device)
            xrec = ae(x).cpu().numpy()
            x_np = x.cpu().numpy()
            # MSE per sample
            mse = np.mean((xrec - x_np)**2, axis=1)
            rec_errors.append(mse)
            metas += list(meta)
    rec_errors = np.concatenate(rec_errors)
    return rec_errors, metas

def plot_training_history(history: dict, out_dir: str, title: str = 'training'):
    os.makedirs(out_dir, exist_ok=True)
    plt.figure()
    plt.plot(history['train_loss'], label='train')
    plt.plot(history['val_loss'], label='val')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.title(f'Loss curve ({title})')
    plt.legend()
    path = os.path.join(out_dir, f'loss_{title}.png')
    plt.savefig(path, dpi=150)
    plt.close()
    print("Saved loss plot to", path)

def save_results_table(df: pd.DataFrame, out_dir: str, name: str = 'results'):
    os.makedirs(out_dir, exist_ok=True)
    path = os.path.join(out_dir, f'{name}.csv')
    df.to_csv(path, index=False)
    print("Saved results table to", path)

# -----------------------------
# High-level pipeline
# -----------------------------
def build_feature_matrix_from_files(file_paths: List[str]) -> Tuple[np.ndarray, List[dict]]:
    """
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º: features (N x F), meta_list
    """
    feats = []
    meta = []
    for p in file_paths:
        try:
            loaded = load_file_generic(p)
            arr = loaded['data']
            f = extract_features_from_sample(arr)
            feats.append(f)
            m = {'path': p}
            m.update(loaded.get('meta', {}))
            meta.append(m)
        except Exception as e:
            print(f"Warning: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å {p}: {e}")
    X = np.vstack(feats).astype(float)
    return X, meta

def prepare_datasets(X: np.ndarray, y: Optional[np.ndarray], test_size: float = 0.2, val_size: float = 0.1, random_state: int = 42):
    if y is None:
        # unsupervised: split train/val/test only X
        X_train, X_temp = train_test_split(X, test_size=test_size+val_size, random_state=random_state)
        val_prop = val_size / (test_size + val_size)
        X_val, X_test = train_test_split(X_temp, test_size=val_prop, random_state=random_state)
        return X_train, X_val, X_test, None, None, None
    else:
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size+val_size, random_state=random_state)
        val_prop = val_size / (test_size + val_size)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_prop, random_state=random_state)
        return X_train, X_val, X_test, y_train, y_val, y_test

# -----------------------------
# Console / CLI
# -----------------------------
def parse_args():
    p = argparse.ArgumentParser(description="Engine model analyzer (MLP / Autoencoder)")
    p.add_argument('--engine', choices=['aviation', 'rocket'], required=False, help='–¢–∏–ø –¥–≤–∏–≥–∞—Ç–µ–ª—è (–∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–π/—Ä–∞–∫–µ—Ç–Ω—ã–π)')
    p.add_argument('--files', nargs='+', help='–§–∞–π–ª—ã —Å –º–æ–¥–µ–ª—è–º–∏ (CSV/JSON/NPZ/MAT/TXT)')
    p.add_argument('--labels', help='CSV —Ñ–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å). –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É path –∏ –∫–æ–ª–æ–Ω–∫—É label (float –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏–ª–∏ int –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)')
    p.add_argument('--task', choices=['regression', 'classification', 'anomaly'], default='anomaly', help='–†–µ–∂–∏–º –∑–∞–¥–∞—á–∏')
    p.add_argument('--out', default='out_results', help='–ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    p.add_argument('--epochs', type=int, default=100, help='–≠–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è')
    p.add_argument('--batch', type=int, default=32, help='batch size')
    p.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', help='cuda or cpu')
    p.add_argument('--seed', type=int, default=42)
    return p.parse_args()

def main():
    args = parse_args()
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    print("Device:", args.device)
    device = torch.device(args.device)

    # Interactive fallback if not provided
    if args.engine is None:
        print("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –¥–≤–∏–≥–∞—Ç–µ–ª—è:\n 1) aviation\n 2) rocket")
        ch = input("–í–≤–µ–¥–∏—Ç–µ 1 –∏–ª–∏ 2: ").strip()
        args.engine = 'aviation' if ch == '1' else 'rocket'

    if not args.files:
        print("–£–∫–∞–∂–∏—Ç–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –º–æ–¥–µ–ª–∏ (—á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª). –ü—Ä–∏–º–µ—Ä: model1.csv model2.mat")
        files_input = input("files: ").strip()
        files = files_input.split()
    else:
        files = args.files

    # Build features
    print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(files)} —Ñ–∞–π–ª–æ–≤...")
    X_raw, metas = build_feature_matrix_from_files(files)
    print("–§–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:", X_raw.shape)

    # If labels provided, load them
    y = None
    if args.labels:
        df_lbl = pd.read_csv(args.labels)
        # expect columns "path" and "label"
        label_map = {row['path']: row['label'] for _, row in df_lbl.iterrows()}
        lbls = []
        for m in metas:
            p = os.path.basename(m['path'])
            # try full path then basename
            lbl = label_map.get(m['path'], label_map.get(p, None))
            if lbl is None:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –º–µ—Ç–∫–∞ –¥–ª—è —Ñ–∞–π–ª–∞ {m['path']} –≤ {args.labels}")
            lbls.append(lbl)
        y = np.array(lbls)
        print("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç–∫–∏, –∑–∞–¥–∞—á–∞:", args.task)

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_raw)

    # Prepare datasets
    X_train, X_val, X_test, y_train, y_val, y_test = prepare_datasets(X_scaled, y, test_size=0.2, val_size=0.1)
    out_dir = os.path.join(args.out, f"{args.engine}_{int(time.time())}")
    os.makedirs(out_dir, exist_ok=True)

    if args.task == 'anomaly':
        # Autoencoder pipeline
        input_dim = X_train.shape[1]
        ae = Autoencoder(input_dim=input_dim, latent_dim=max(4, input_dim // 4), hidden_dims=[max(32, input_dim//2)], activation='relu')
        train_ds = EngineDataset(X_train)
        val_ds = EngineDataset(X_val)
        test_ds = EngineDataset(X_test)
        train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=args.batch, shuffle=False)
        test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)
        print("Training autoencoder...")
        ae, history = train_autoencoder(ae, train_loader, val_loader, device, epochs=args.epochs, lr=1e-3, early_stop_patience=10, checkpoint_path=os.path.join(out_dir, 'ae_best.pth'))
        plot_training_history(history, out_dir, title='autoencoder')
        # compute reconstruction errors
        rec_train, _ = evaluate_autoencoder(ae, train_loader, device)
        rec_val, metas_val = evaluate_autoencoder(ae, val_loader, device)
        rec_test, metas_test = evaluate_autoencoder(ae, test_loader, device)
        # threshold (e.g. mean + 3*std on validation)
        thr = rec_val.mean() + 3 * rec_val.std()
        print(f"Threshold for anomaly (val mean+3std): {thr:.6f}")
        # report
        df_res = pd.DataFrame({
            'path': [m['path'] for m in metas_test],
            'reconstruction_error': rec_test,
            'anomaly': rec_test > thr
        })
        save_results_table(df_res, out_dir, name='anomaly_results')
        # plot histogram
        plt.figure()
        plt.hist(rec_test, bins=50)
        plt.axvline(thr, color='r', linestyle='--', label='threshold')
        plt.xlabel('reconstruction error')
        plt.ylabel('count')
        plt.title('Anomaly scores (test)')
        plt.legend()
        plt.savefig(os.path.join(out_dir, 'anomaly_hist.png'), dpi=150)
        plt.close()
        print("Autoencoder pipeline finished. Results in", out_dir)
    else:
        # Supervised pipeline
        if y is None:
            raise ValueError("–î–ª—è supervised —Ä–µ–∂–∏–º–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å --labels")
        # determine output dim
        if args.task == 'regression':
            out_dim = 1
            loss_task = 'regression'
        else:
            # classification: detect number of classes
            classes = np.unique(y)
            out_dim = len(classes)
            loss_task = 'classification'
            # if classes not starting at 0, map
            if not np.array_equal(classes, np.arange(len(classes))):
                mapping = {v: i for i, v in enumerate(classes)}
                y_train = np.array([mapping[v] for v in y_train])
                y_val = np.array([mapping[v] for v in y_val])
                y_test = np.array([mapping[v] for v in y_test])
        input_dim = X_train.shape[1]
        hidden_dims = [max(128, input_dim*2), max(64, input_dim)]
        model = MLP(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=out_dim, activation='relu', dropout=0.1)
        # Datasets and loaders
        train_ds = EngineDataset(X_train, y_train, meta=[None]*len(X_train))
        val_ds = EngineDataset(X_val, y_val, meta=[None]*len(X_val))
        test_ds = EngineDataset(X_test, y_test, meta=[None]*len(X_test))
        train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=args.batch, shuffle=False)
        test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)

        print("Training supervised model...")
        model, history = train_supervised(model, train_loader, val_loader, device, epochs=args.epochs, lr=1e-3, task=loss_task, early_stop_patience=10, checkpoint_path=os.path.join(out_dir, 'super_best.pth'))
        plot_training_history(history, out_dir, title='supervised')
        preds, targets, metas_test = evaluate_supervised(model, test_loader, device, task=loss_task)

        # process outputs
        if args.task == 'regression':
            preds_flat = preds.squeeze()
            mse = mean_squared_error(targets, preds_flat)
            r2 = r2_score(targets, preds_flat)
            print(f"Test MSE: {mse:.6f}, R2: {r2:.6f}")
            df_res = pd.DataFrame({
                'path': [m['path'] if m else '' for m in metas_test],
                'target': targets,
                'pred': preds_flat,
                'abs_error': np.abs(targets - preds_flat)
            })
            save_results_table(df_res, out_dir, name='regression_results')
            # plot predicted vs true
            plt.figure()
            plt.scatter(targets, preds_flat, alpha=0.7, s=20)
            plt.plot([targets.min(), targets.max()], [targets.min(), targets.max()], 'r--')
            plt.xlabel('true')
            plt.ylabel('pred')
            plt.title('Prediction vs True (test)')
            plt.savefig(os.path.join(out_dir, 'pred_vs_true.png'), dpi=150)
            plt.close()
        else:
            pred_labels = preds.argmax(axis=1)
            acc = accuracy_score(targets, pred_labels)
            print(f"Test accuracy: {acc:.4f}")
            # For binary, compute AUC if possible
            auc = None
            try:
                if preds.shape[1] == 2:
                    auc = roc_auc_score(targets, preds[:,1])
                    print("AUC:", auc)
            except Exception:
                auc = None
            df_res = pd.DataFrame({
                'path': [m['path'] if m else '' for m in metas_test],
                'target': targets,
                'pred_label': pred_labels
            })
            save_results_table(df_res, out_dir, name='classification_results')
        print("Supervised pipeline finished. Results in", out_dir)
    # –ø—Ä–∏–º–µ—Ä: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∑–∞–¥–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø—É—Ç–∏)
    res = prepare_cmapss_supervised(
        train_path='data/train_FD001.txt',
        test_path='data/test_FD001.txt',
        test_rul_path='data/RUL_FD001.txt',
        n_settings=3,
        window_size=30,
        step=1,
        sensors_only=True,
        feature_extractor=extract_time_series_features,
        scale=True
    )

    X_train, y_train = res['X_train'], res['y_train']
    X_test, y_test = res['X_test'], res['y_test']
    meta_test = res['meta_test']
    # –∑–∞—Ç–µ–º –æ–±—ë—Ä–Ω—É—Ç—å –≤ EngineDataset –∏ DataLoader:
    train_ds = EngineDataset(X_train, y_train, meta=[None]*len(X_train))
    test_ds = EngineDataset(X_test, y_test, meta=meta_test)


    # Optional: PCA visualization of feature space
    try:
        pca = PCA(n_components=min(3, X_scaled.shape[1]))
        X_p = pca.fit_transform(X_scaled)
        plt.figure()
        if X_p.shape[1] == 3:
            ax = plt.axes(projection='3d')
            ax.scatter(X_p[:,0], X_p[:,1], X_p[:,2], s=10)
            ax.set_title('PCA 3D of feature space')
        else:
            plt.scatter(X_p[:,0], X_p[:,1], s=10)
            plt.title('PCA 2D of feature space')
        plt.savefig(os.path.join(out_dir, 'pca_features.png'), dpi=150)
        plt.close()
    except Exception as e:
        print("PCA visualization failed:", e)

    print("Done. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã -- –≤ –ø–∞–ø–∫–µ:", out_dir)
    print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ–≤—ã—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞: —É–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä—ë–º –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, —É–ª—É—á—à–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É (—Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏), –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω—Å–∞–º–±–ª–∏, –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –∏ hyperparameter tuning (Optuna/Random Search).")
    # –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ df_train –∏ df_test —Å RUL (soft-cap —É–∂–µ –ø—Ä–∏–º–µ–Ω—ë–Ω)
    X_tr, y_tr, meta_tr = build_cmapss_window_dataset_gpu(df_train, window_size=30, step=1, sensors_only=True, batch_size=2048)
    X_te, y_te, meta_te = build_cmapss_window_dataset_gpu(df_test, window_size=30, step=1, sensors_only=True, batch_size=2048)
    # –∑–∞—Ç–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
    scaler = StandardScaler()
    X_tr = scaler.fit_transform(X_tr)
    X_te = scaler.transform(X_te)

if __name__ == '__main__':
    main()
